Тестовое задание МТС
Гольдовский Илья Валерьевич, ITFB Group

0. К работе в офисе готов.
1. Файл базы данных sqllite 'tweets.db' размещен в директории проекта.
2. Атрибут 'tweet_sentiment' заведен в таблице 'tweets_db' в модуле main.py.
3. Нормализация хранения твита выполнена в файле 'normalization.sql'.
4. Эмоциональня окраска сообщения (средний sentiment) рассчитывается в модуле 'afinn.py' и добавляется в таблицу
   'tweets_db_normalized'.
5. Скрипты представлены в файле 'analysis.sql'.
   Результаты выполнения скриптов представлены в файле 'Результаты выполнения SQL-скриптов (п.5).xlsx'.

6. Ежедневный анализ согласно п.5

Компоненты решения
- Apache Airflow для планирования и организации ETL-процесса.
- PostgreSQL для промежуточной обработки загруженных таблиц.
- HDFS/Hive для хранения обработанных таблиц.
- Hive как OLAP-система для аналитики по сохраненным данным.
- Tableau/QlikView для создания Витрин данных.


Шаги ETL-процесса
1. Планирование ETL-процесса организовано на базе Apache Airflow (загрузка json-файла каждые три минуты).
2. Файл загружается в PostgreSQL в формате из п.1., к таблице добавляется столбец "tweet_sentiment" согласно п.2.
3. Таблица проходит нормализацию согласно п.3. (создаются новые нормализованные таблицы tweets_db_normalized и location).
4. Для нормализованных таблиц выполняется расчет "sentiment" согласно п.4.
5. Нормализованные таблицы с рассчитанными "sentiment" сохраняются в Hive на базе файловой системы HDFS.
6. Над Hive создаются витрины данных на Tableau/QlikView согласно запросам пользователей.
