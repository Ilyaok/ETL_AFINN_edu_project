Тестовое задание МТС
Гольдовский Илья Валерьевич, ITFB Group

0. К работе в офисе готов.
1. Файл база данных sqllite размещен в директории проекта.
2. Все .py и .sql скрипты размещены в директории проекта.

3. Ежедневный анализ согласно п.5

Компоненты решения
- Apache Airflow для планирования и организации ETL-процесса.
- PostgreSQL для промежуточной обработки загруженных таблиц.
- HDFS/Hive для хранения обработанных таблиц.
- Hive как OLAP-система для аналитики по сохраненным данным.
- Tableau/QlikView для создания Витрин данных.


Шаги ETL-процесса
1. Планирование ETL-процесса организовано на базе Apache Airflow (загрузка json-файла каждые три минуты).
2. Файл загружается в PostgreSQL в формате из п.1., к таблице добавляется столбец "tweet_sentiment" согласно п.2.
3. Таблица проходит нормализацию согласно п.3. (создаются новые нормализованные таблицы tweets_db_normalized и location).
4. Для нормализованных таблиц выполняется расчет "sentiment" согласно п.4.
5. Нормализованные таблицы с рассчитанными "sentiment" сохраняются в Hive на базе файловой системы HDFS.
6. Над Hive создаются витрины данных на Tableau/QlikView согласно запросам пользователей.
